---
title: "How will the stock market react to the PCA: Envidence From Yahoo Finance Stock Market"
author:
- familyname: Jin
  othernames: Kaiwen
  address: Monash University 
  email: kjin7@student.monash.edu
  correspondingauthor: true
  qualifications: 26686953
- familyname: Zhang
  othernames: Zhiruo
  address: Monash University
  email: zzha0001@student.monash.edu
  correspondingauthor: true
  qualifications: 28009487
- familyname: Luo
  othernames: Jinhao
  address: Monash University
  email: jluo0015@student.monash.edu
  correspondingauthor: true
  qualifications: 29012449
department: Department of\newline Econometrics &\newline Business Statistics
organization: ETF5500 Assignment2
bibliography: references.bib
biblio-style: apa
linestretch: 1.5
output:
  MonashEBSTemplates::report:
    fig_caption: yes
    fig_height: 5
    fig_width: 8
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
    toc: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache=FALSE, messages=FALSE, warning=FALSE)
# Make sure you have the latest versions of rmarkdown and bookdown installed
library(dplyr)
library(ggplot2)
library(tidyverse)
library(mclust)
library(visdat)
library(gridExtra)
library(kableExtra)
library(tibble)
```

\clearpage

# Introduction

In the financial market, the value of stocks would be investigated by many different variables. However, a large number of stocks make investors hard to make their decision. Therefore, this report would apply linear combination (LC) by combining the variables into an index and utilise principal component analysis (PCA) to evaluate the performance of stocks. 

Besides, this report will also consider the accuracy of PCA and will discuss the potential limitation of PCA in stocks performance evaluation. Based on the result, Clustering Analysis as a comparative approach will also be provided.  

At last, some useful suggestions for the stocks choosing will be concluded, as well as concluding the biases generated from the analysis.

The appendix will contain some notes to improve the understanding of our reports.

# Data Description

```{r data, message=FALSE}
stocks_ori <- read_csv("stocks.csv")
#nrow(stocks)
#head(stocks)
stocks_ori <- stocks_ori%>%
  rename(intra_day=`Market cap (intra-day)`,
         ent_value=`Enterprise value`,
         trail_pe=`Trailing P/E`,
         for_pe=`Forward P/E`,
         peg=`PEG ratio (5-yr expected)`,
         ttm=`Price/sales (ttm)`,
         mrq=`Price/book (mrq)`,
         rev=`Enterprise value/revenue`,
         ebitda=`Enterprise value/EBITDA`,
         tot_risk = `Total ESG risk score`,
         envir_risk = `Environmental Risk Score`,
         social_risk = `Social Risk Score`,
         gover_risk = `Governance Risk Score`)
```

## Description

Our data was sourced from [Yahoo Finance](https://au.finance.yahoo.com/) and it contains 18 variables of 147 stocks from five major financial indices. Those 18 variables could be further classified into 3 categories. The first categories capture **Name**, **Symbol**, **Market**, **Sector**, **Industry** which are related to the background of those stocks. The second and third categories provide some measurement of the value and risk which are related to the stocks. The further description of those variables is shown in Table \@ref(tab:variables-table). 

```{r variables-table, message=FALSE}
tribble(~Variable, ~Abbreviation, ~Description,
        "Name", "/", "The full company name of each stock",
        "Symbol", "/", "The abbreviation of each stock",
        "Market", "/", "Major financial indices",
        "Sector", "/", "The belonging section of a stock",
        "Industry", "/", "The belonging industry of a stock",
        "Market capitalization", "intra_day", "How much a company is worth as determined by the stock market", 
        "Enterprise value", "ent_value", "A measure of a company's total value", 
        "Trailing P/E", "trail_pe",  "Price to Earning Ratio based on the earnings per share over the previous 12 months",
        "Forward P/E ratio", "for_pe",  "Estimate further earnings per share in the next 12 months", 
        "PEG ratio", "peg", "Enhances the P/E ratio by adding the expected earnings growth into calculation", 
        "P/S ratio", "ttm", "Price to Sales ratio, a valuation ratio by comparing a companyâ€™s stock price to its revenue", 
        "P/B ratio",  "mrq", "Price to Book ratio is a measurement of the market's valuation of a company relative to its book value",
        "Enterprise value-to-revenue",  "rev", "Also refers as the EV/R, it measures the value of a stock that compares a company's enterprise value to its revenue", 
        "EV/EBITDA", "ebitda", "Enterprise value to earnings before interest, taxed, depreciation and amortization ratio compares the value of a company, debt included to the company's cash earnings less non-cash expenses", 
        "Total ESG risk score", "tot_risk", "The overall rating scores based on the Morningstar Sustainability Rating systems", 
        "Environmental Risk Score", "envir_risk", "Evaluation scores of the portfolios performance when they meet the environmental challenges", 
        "Social Risk Score", "social_risk", "Evaluation scores of the portfolios performance when they meet the social challenges", 
        "Governance Risk Score", "gover_risk","Evaluation scores of the portfolios performance when they meet the governance challenges") %>% 
  kable(caption = "Information of variables of the original data") %>% 
  kable_styling(bootstrap_options = c("bordered", "striped", "hover")) %>%
  column_spec(3, width = "170px") 
```

## Limitation

This part will provide us an introduction about the limitation of the dataset, and it is shown below:

- This dataset contains a lot of missing value which would cause some bias in our final result
- This dataset does not contain enough observations. The insufficient sample space will make our final result become unreliable. Also, if we further filter out the missing values, the sample size of the data would be even smaller. And the relatively small sample would not be representative enough to clarify the overall condition. 
- There is some inconsistency between the total ESG risk score and the sum of individual risk score. This inconsistency would directly increase the error in our final output. 

Those limitations would be further discussed in the following sections. At last, the biases of analysis which generate from the limitations would be concluded.

# Analysis 

## Preliminary Analysis 
We will tidy our original dataset by removing the missing variables and further figure out other features. Figure \@ref(fig:vis-data) shows the general data structure and it could be classified into three types which are character, numeric and missing value.

```{r vis-data, fig.cap="The data structure of original data"}
vis_dat(stocks_ori)
```

Table \@ref(tab:summary-table) indicates that the initial 147 observations have up to 102 missing value and also some potential outliers.

```{r sum-tab, eval = FALSE}
sum_tab<-stocks_ori%>%
   select(-Name,-Symbol,-Market,-Sector,-Industry)%>%
   summary(stocks_ori)
```

```{r summary-table}
tribble(~Variable, ~Min, ~Median, ~Mean, ~Max, ~`NA`,
        "intra_day","-2","63","95065","5110000","NA",
        "ent_value" ,"-264","70","85683","5130000","NA",
        "trail_pe" ,"0.48","20.11","43.62","1479.29", "18",
        "for_pe","3.59","19.92","43.04","1044.81", "80",
        "peg" ,"-62.380","2.405","15.223","713.670","81",
        "ttm" ,"0.9","2.8","9.941","548.150","17",
        "mrq" ,"0.1","5.4","174.16","11765.96","10",
        "rev" ,"-27.720","2.875","9.827","5411.160","17",
        "ebitda" ,"-465.460","13.765","19.461","1117.510","23",
        "tot_risk","11","23","25.39","75","13",
        "envir_risk","0","4","6.731","62", "13",
        "social_risk","3","10","11.4","88","13",
        "gover_risk","3","8" ,"9.343","80","13") %>%
  kable(caption = "Summary table of original data") %>%
  kable_styling(bootstrap_options = c("bordered", "striped", "hover"))
```

Most of the variables have a small median and mean, but an extremely high maximum value. Those extreme value would dominate our Principle Component Analysis and those outliers are shown in Table \@ref(tab:outliers-table).


```{r outliers-table}
tribble(~variable, ~outlier,
        "intra_day", "MSFT, AAPL",
        "ent_value", "MSFT, AAPL",
        "trail_pe", "TSLA",
        "for_pe", "ILMN, TSLA",
        "peg", "DIS, VZ, KO, MMM, CVX, PCAR, CAT, XOM",
        "ttm", "ILMN, V",
        "mrq", "TSLA",
        "rev", "ILMN, V",
        "ebitda", "INTU, ILMN, TSLA, NKE") %>%
  kable(caption = "The summary table of outliers in each variables") %>%
  kable_styling(bootstrap_options = c("bordered", "striped", "hover"))
```

New dataset **stocks** will be generated by removing the missing value. 

```{r stocks}
stocks <- na.omit(stocks_ori)
```

## Principle Component Analysis

### Value Analysis 

Value analysis will be conducted by removing the high influential outliers. We also need to standardize the data due to the different units. 

```{r value}
# filter out the outliers
issuer<-stocks%>%
  select(Symbol,intra_day,ent_value,trail_pe,for_pe,peg,ttm,mrq,rev,ebitda)
df <- issuer[!(issuer$Symbol %in% c("MSFT", "AAPL", "TSLA", "ILMN", "DIS", "VZ", "KO", "MMM", "CVX", "PCAR", "CAT", "XOM", "V", "INTU", "NKE")),]
#view(df)
```

```{r pca-value}
pca_value<-df%>%
  select_if(is.numeric)%>%
  prcomp(scale.=TRUE)
rownames(pca_value$x)<-pull(df,Symbol)
```

```{r pca-cor,fig.height=5.5,fig.cap="Correlation Biplot of PCA of stocks' value"}
cor_va<-biplot(pca_value,scale=0,cex=0.6)
```

Referring to the PCA value table, we could notice that PC1 is positive correlated with the measurement of company value which is **intra_day** and **ent_value**. PC2 is positive correlated with the stock earnings ratio (**ebitda** and **trail_pe**) which means that the increase in the stock earnings ratio will increase PC2 slightly. We could notice that the variables which are related to the price-based evaluation of the stock are pretty close to the PC2.

Meanwhile, according to Figure \@ref(fig:pca-cor) the narrow angle between **intra_day** and **ent_value** could highlight their strong association and the nearly 90 degree angle with **for_eg**,**ttm**, **mrq**, **rev** states the zero relationship. Furthermore, we could say that the angle between the measurement of company value and **ebita**, **trail_pe** is close to 180 degree, their relationship should be negative. In other words, market value of a company will not influence its stock price and earn per share, but price and earnings will highly impact each other. 

```{r bi-dis,fig.height=5.5,fig.cap="Distance Biplot of PCA of stocks' value"}
dis_va<-biplot(pca_value,xlim=c(-0.6,0.3),cex=0.6)
``` 

Distance biplot \@ref(fig:bi-dis) indicates that **Johnson & Johnson (JNJ)** and **Walmart (WMT)** have a high value in PC1, and **Activision Blizzard (ATVI)**, **Texas Instruments Incorporated (TXN)**, **Maxim Integrated Products (MXIM)** are higher in PC2. 

Furthermore, we notice that **Verisk analytics (VRSK)** is a potential outlier for the PC1, **JNJ**, and **WMT** are potential outliers for PC2. The reason is based on the characteristic of these firms. Being a data-analytics and risk-assessment firm, **VRSK** provides the consulting service instead of the goods selling. Therefore, as for the financial sector, they profit generating will not directly lead to the increasing firm size. **JNJ** and **WMT** perform oppositely since they are operating as the multinational company. The continuously increasing market share will keep their profit at a high level. 
 
```{r lim-1}
summary(pca_value)$importance %>%  
  round(4) %>% 
  kable(caption = "Summary table of PCA for value analysis of stocks") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped")) %>%
  column_spec(1, width = "70px") 
```

```{r lim-2, fig.cap="Screeplot of PCs in PCA for value analysis of stocks"}
screeplot(pca_value, type = "lines")
```

The limitation of the value analysis also exists. 

-	After we filter out the outliers, the number of variables we put into use is 30 out of 147 and only 70.42% of the overall variation could be explained by the first two principles (Table \@ref(tab:lim-1)). The really small space size is not representative and also would not accurate enough to explain the whole stock market condition. 
-	Screeplot (Figure \@ref(fig:lim-2)) suggests the best PC selection is 3 which is contradictory to the biplot selection. 

Therefore, we need to consider an alternative approach to make sure of the accuracy of our suggestion.  

```{r, eval=FALSE}
stocks %>% filter(Industry == "Technology")
```

### Risk Analysis

This part will provide a discussion about the potential risk of each stock based on the ESG risk score. We will compare the total risk score with the sum of the ESG scores to make sure the consistency of our data. Filtering out the inconsistent value would improve the accuracy of our results.

Table \@ref(tab:pca-risk-summary) that PC1 and PC2 have explained almost 86% of the total variation of 4 variables. Besides, Figure \@ref(fig:pca-risk-screeplot) also suggests that the principal component of one and two should be selected because they all with a variance greater than 1 according to the Kaiser's Rule.

```{r risk-df}
# filter out the outliers
risk <- subset(stocks[(stocks$Symbol %in% df$Symbol),], select = c("Symbol", "tot_risk", "envir_risk", "social_risk", "gover_risk"))
#view(risk)
```

```{r first-check}
d <- apply(cbind(risk$envir_risk, risk$social_risk, risk$gover_risk), 1, sum)
#all(risk$tot_risk == d)
```

```{r risk-data}
# checking if total esg risk score = environmrnt + social + government 
risk_tesg <- risk[risk$tot_risk == d,]
d2 <- apply(cbind(risk_tesg$envir_risk, risk_tesg$social_risk, risk_tesg$gover_risk), 1, sum)
#all(risk_tesg$tot_risk == d2)
#head(risk_tesg)
#nrow(risk_tesg)
```

```{r principal-component-analysis}
pca_risk <- risk_tesg %>% select_if(is.numeric) %>% prcomp(scale. = TRUE)
```

```{r pca-risk-summary}
summary(pca_risk)$importance %>%  
  round(4) %>%
  kable(caption = "Summary table of PCA for risks analysis of stocks") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped"))
```

```{r pca-risk-screeplot, fig.cap="Screeplot of PCs in PCA for risk analysis of stocks"}
screeplot(pca_risk, type = "lines")
```

Figure \@ref(fig:pca-risk-distance) shows the distance among each stock in the dataset and implies the similarity between stocks. The stocks of **VRSK** and **UnitedHealth Group Incorporated (UNH)** may be the same because they seem perfectly superimpose. Besides, **Allianz SE (ALV.DE)** and **Dollar Tree, Inc. (DLTR)**, as well as **Cerner Corporation (CERN)** and **Fiserv, Inc. (FISV)** might be similar, since they are close to each other. While the far distance between **VRSK** and **Microchip Technology Incorporated (MCHP)**, or **CDW Corporation (CDW)** and **Pfizer Inc. (PFE)** indicate their different risk performance. To further analyzing the correlation between each stock, a correlation biplot is required.

```{r pca-risk-distance, fig.height=5.5, fig.cap= "Distance biplot of PCA of stocks' risk"}
rownames(pca_risk$x) <- pull(risk_tesg, Symbol)
biplot(pca_risk, xlim=c(-0.5, 0.5), ylim = c(-0.6, 0.3),  cex = 0.6)
```

According to Figure \@ref(fig:pca-risk-correlation) the correlation biplot **VRSK**, **UNH**, **CERN**, and **FISV** have the high values of social risk score, which indicate that these four stocks might perform better than the others in the social challenges. While they might not good at dealing with the risk from the environment since the social risk score and environmental risk score are approximately negative correlated. In contrast, **Covestro AG (1COV.DE)** and **MCHP** have the strongest abilities to face environmental challenges. Meanwhile, **MCHP** also has the highest score of governance risk, which indicates good performance in anticipating governance challenges. **PFE**, **Bayerische Motoren Werke AG (BMW.DE)**, and **Merck & Co., Inc. (MRK)** also perform well. While the projected positions of these three stocks along the axis of governance risk score are gradually decreasing, the approximate actual values of stock performance might gradually decline.

In general, based on the total ESG risk score, **MCHP** and **BMW.DE** have the best overall performance compared with other stocks, which indicate that their strategy in risk management is quite effective. Therefore, even if some external challenges occur, they will not have any significant fluctuations. In contrast, **CDW** has a weak overall risk performance for the low value in total risk score which indicates that the **CDW** stock price will not be stable when facing the risk. 

```{r pca-risk-correlation, fig.height=5.5, fig.cap="Correlation Biplot of PCA of stocks' risk"}
biplot(pca_risk, scale = 0, xlim = c(-4, 4), cex = 0.6)
```

```{r, eval=FALSE}
summary(pca_risk)
pca_risk$x 
pca_risk$rotation
```

```{r pca-risk-observations, eval=FALSE}
pca_risk$x %>%
  kable(caption = "") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped"))
```

```{r pca-risk-weights, eval=FALSE}
pca_risk$rotation %>%
  kable(caption = "") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped"))
```

## Cluster Analysis

Using the hierarchical clustering analysis with the agglomerative method, it is a bottom-up approach. We first select the data set that is consistent with the value data by equivalent stocks symbol. After standardised the data for the numeric variables, the Euclidian distance applied to find the distance between all pairs of observations. We employ Ward's methodology to sort the clusters. And the resulting clusters are shown in the dendrogram to display the sequences of merges or splits. Figure \@ref(fig:ward) claims that two and four clusters of solutions are not stable. Hence, the three cluster solution is stable which is shown in \@ref(fig:ward-3).

```{r dat-cluster}
dat <- stocks[(stocks$Symbol %in% df$Symbol),]
d <- dat %>% select_if(is.numeric) %>% scale %>% dist 
```

```{r ward, fig.cap="Choosing clusters"}
hc_w <- hclust(d, method = "ward.D2")
hc_w$labels <- dat$Symbol
par(mfrow = c(1,3))
plot(hc_w, cex = 0.5, xlab= "Stocks", ylab="Distance")
rect.hclust(hc_w, k = 2)
plot(hc_w, cex = 0.5, xlab= "Stocks", ylab="Distance")
rect.hclust(hc_w, k = 3)
plot(hc_w, cex = 0.5, xlab= "Stocks", ylab="Distance")
rect.hclust(hc_w, k = 4)
```

```{r ward-3, fig.cap="Dendogram using Ward methodology and taking Euclidian distances"}
plot(hc_w, cex = 0.5, xlab= "Stocks", ylab="Distance")
rect.hclust(hc_w, k = 3)
```

From the dendrogram, there are three different clusters. These three clusters are shown in Table \@ref(tab:memb-w), Table \@ref(tab:memb-two), and Table \@ref(tab:memb-three), respectively.

```{r memb-w}
memb_three_w <- cutree(hc_w, k = 3)
memb_one <- dat[dat$Symbol %in% rownames(as.data.frame(memb_three_w) %>% filter(memb_three_w == 2)),]$Name

memb_one %>% 
  kable(col.names = c("stock"), caption = "The stocks of the first cluster") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped")) 
  
```

```{r memb-two}
memb_two <- dat[dat$Symbol %in%  rownames(as.data.frame(memb_three_w) %>% filter(memb_three_w == 3)),]$Name
memb_two %>%
  kable(col.names = c("stock"), caption = "The stocks of the second cluster") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped")) 
```


```{r memb-three}
memb_three <- dat[dat$Symbol %in%  rownames(as.data.frame(memb_three_w) %>% filter(memb_three_w == 1)),]$Name
memb_three %>%
  kable(col.names = c("stock"), caption = "The stocks of the third cluster") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped")) 
```

\clearpage

# Conclusions 

According to our general evaluation of the Yahoo Finance market, we could say that **JNJ** and **WMT** perform better in company evaluation while **ATVI**, **TXN**, and **MXIM** are better in price and earning.  **VRSK**, **JNJ** and **WMT** could be considered as the special cases since they outperform in their area. What ESG risk analysis provides us is that **MCHP** and **BMW.DE** have a great performance in overall anti-risk, some other companies perform better in a specific risk score. For instance, **UNH**, **CERN**, and **FISV** perform well in anti-social risk, but they are not resilient enough when meeting the challenges from environmental risk compared with **1COV.DE** and **MCHP**. And our investment suggestions are listed below:

- Fully consider the characteristics of the firm and consider the factors which might dominate the stock value. 
- Both internal and external risks would on the value of stocks and will generate the fluctuation of prices in the stock market as well. 
- Investors need to make the investment decision based on their risk tolerance and well balance differences in the risk-control of each company. 
- Companies need to improve the ability of self-resilience and anti-risks so than enhance the performance when facing different types of risks.

\clearpage

# Acknowledgement

The data could be downloaded from [Yahoo Finance](https://au.finance.yahoo.com/). Meanwhile, the report uses the template called **Monash Consulting Report** which could use by downloading the package called [MonashEBSTemplates](https://github.com/robjhyndman/MonashEBSTemplates). In addition, the programming language used to analyse the stocks is R (4.0.2) (R Core Team, 2020).

Following packages has been included in our Rmd file:

- package dplyr (1.0.1) (Wickham et al., 2020),
- package ggplot2 (3.3.2)  (Wickham, 2016),
- package tidyverse (1.3.0)  (Wickham et al., 2019),
- package mclust (5.4.6)  (Scrucca et al., 2016),
- package visdat (0.5.3)  (Tierney, 2017),
- package gridExtra (2.3)  (Auguie, 2017),
- package kableExtra (1.1.0)  (Zhu, 2019),
- package tibble (3.0.3)  (MÃ¼ller & Wickham, 2020).

\clearpage


# References

Auguie, B. (2017). Gridextra: Miscellaneous functions for "grid" graphics [R package version 2.3].
    https://CRAN.R-project.org/package=gridExtra
    
MÃ¼ller, K, & Wickham, H. (2020). Tibble: Simple data frames [R package version 3.0.3].                        https://CRAN.R-project.org/package=tibble

R Core Team. (2020). R: A language and environment for statistical computing. R Foundation for                Statistical Computing. Vienna, Austria. https://www.R-project.org/

Scrucca, L, Fop, M, Murphy, TB, & Raftery, AE. (2016). mclust 5: Clustering, classification and density
    estimation using Gaussian finite mixture models. The R Journal, 8(1), 289â€“317.
    
Tierney, N. (2017). Visdat: Visualising whole data frames. JOSS, 2(16), 355.

Wickham, H. (2016). Ggplot2: Elegant graphics for data analysis. Springer-Verlag New York.                  https://ggplot2.tidyverse.org

Wickham, H, Averick, M, Bryan, J, Chang, W, McGowan, LD, FranÃ§ois, R, Grolemund, G, Hayes, A,
    Henry, L, Hester, J, Kuhn, M, Pedersen, TL, Miller, E, Bache, SM, MÃ¼ller, K, Ooms, J, Robinson,
    D, Seidel, DP, Spinu, V, ... Yutani, H. (2019). Welcome to the tidyverse. Journal of Open
    Source Software, 4(43), 1686.
    
Wickham, H, FranÃ§ois, R, Henry, L, & MÃ¼ller, K. (2020). Dplyr: A grammar of data manipulation [R
    package version 1.0.1]. https://CRAN.R-project.org/package=dplyr
    
Zhu, H. (2019). Kableextra: Construct complex table with â€™kableâ€™ and pipe syntax [R package version
    1.1.0]. https://CRAN.R-project.org/package=kableExtra


\clearpage

\appendix

# Appendix

## Ends with Emphasis 

At the end of our report, it is necessary to emphasis that due to the small sample space and the incomplete eigenvalue selection, our result might not be representative and the biplot could not fully state the overall situation. Even though, in our case, the biplot is suitable for risk analysis. We still could not deny the fact that in general, the small sample size would lead to the bias in output. Therefore, our report uses cluster analysis as an alternative approach. The agglomerative method indicates that a stable solution is three clusters. And here we would show complete linkage method (Figure \@ref(fig:complete)), average linkage (Figure \@ref(fig:average)) and centroid method (Figure \@ref(fig:centroid)). In order to check the robustness, we compute the adjusted rand index using `adjustedRandIndex` function. Table \@ref(tab:adjusted-index) indicates that the complete linkage method has a relatively high level of agreement with Ward's method. 

```{r complete, fig.cap="Cluster dendrogram of complete linkage method"}
hc_c <- hclust(d, method = "complete")
hc_c$labels <- dat$Symbol
plot(hc_c, cex = 0.5)
rect.hclust(hc_c, k = 3)
```

```{r memb-complete}
memb_three_c <- cutree(hc_c, k = 3) 
```

```{r average, fig.cap="Cluster dendrogram of average linkage method"}
hc_al <- hclust(d, method = "average")
hc_al$labels <- dat$Symbol
plot(hc_al, cex = 0.5)
rect.hclust(hc_al, k = 3)
```

```{r memb_al}
memb_three_al <- cutree(hc_al, k = 3)
```

```{r centroid, fig.cap="Cluster dendrogram of centroid method"}
hc_cm <- hclust(d, method = "centroid")
hc_cm$labels <- dat$Symbol
plot(hc_cm, cex = 0.5)
rect.hclust(hc_cm, k = 3)
```

```{r memb_cm}
memb_three_cm <- cutree(hc_cm, k = 3)
```

```{r adjusted-index}
c <- adjustedRandIndex(memb_three_w, memb_three_c)
al <- adjustedRandIndex(memb_three_w, memb_three_al)
cm <- adjustedRandIndex(memb_three_w, memb_three_cm)


al_method <- rbind(c, al, cm) 
rownames(al_method) <- c("complete linkage method", "average linkage method", "centroid method")
al_method %>% 
  kable(col.names = c("adjusted rand index"), 
        caption = "The adjusted rand index of the three clustering methods") %>%
  kable_styling(bootstrap_options = c("bordered", "hover", "striped")) 
```

